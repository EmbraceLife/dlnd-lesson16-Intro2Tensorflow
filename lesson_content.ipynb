{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "- In TensorFlow, data isn’t stored as integers, floats, or strings. \n",
    "- These values are encapsulated in an object called a tensor. \n",
    "- In the case of `hello_constant = tf.constant('Hello World!')`, `hello_constant` is a 0-dimensional string tensor\n",
    "\n",
    "----\n",
    "> but tensors come in a variety of sizes as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([ [123,456,789] ]) \n",
    " # C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A # only show class and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(1, 3) dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_3:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Tensor\n",
    "- tf.constant() is one of many TensorFlow operations you will use in this lesson. \n",
    "- The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes.\n",
    "\n",
    "\n",
    "## Session\n",
    "- TensorFlow’s api is built around the idea of **a computational graph**\n",
    "- a way of **visualizing a mathematical process**. \n",
    "\n",
    "----\n",
    "> Let’s take the TensorFlow code you ran and turn that into a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAGUCAYAAAAxhDrBAAAABmJLR0QA/wD/AP+gvaeTAAAACXBI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\ntSlYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('https://d17h27t6h515a5.cloudfront.net/topher/2016/October/580feadb_session/session.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Session\n",
    "- A \"TensorFlow Session\", as shown above, is an environment for running a graph. \n",
    "- The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. \n",
    "\n",
    "\n",
    "----\n",
    "> Let’s see how you use it.\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "```\n",
    "- The code has already created the tensor, hello_constant, from the previous lines. \n",
    "- The next step is to evaluate the tensor in a session.\n",
    "- The code creates a session instance, `sess`, using `tf.Session`. \n",
    "- The `sess.run()` function then evaluates the tensor and returns the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "- In the last section, you passed a tensor into a session and it returned the result. \n",
    "- What if you want to use a non tensor? Empty tensor? \n",
    "- This is where tf.placeholder() and feed_dict come into place. \n",
    "\n",
    "----\n",
    "> In this section, you'll go over the basics of feeding data into TensorFlow.\n",
    "\n",
    "----\n",
    "> **tf.placeholder()**\n",
    "- Sadly you can’t just set x to your dataset and put it in TensorFlow, \n",
    "- because over time you'll want your TensorFlow model to take in different datasets with different parameters. \n",
    "- You need tf.placeholder()!\n",
    "- tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, \n",
    "- allowing you to set the input right before the session runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session’s feed_dict\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "```\n",
    "    \n",
    "> Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. \n",
    "- The above example shows the tensor x being set to the string \"Hello, world\". \n",
    "\n",
    "----\n",
    "> It's also possible to set more than one tensor using feed_dict as shown below.\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "```    \n",
    "   \n",
    "> Note: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type\n",
    "- you’ll get the error “ValueError: invalid literal for...”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess: # try to set x, y, z to take each input\n",
    "    output = sess.run(z, feed_dict={x: 'Test String', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(45.66999816894531, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed the x tensor 123\n",
    "        output = sess.run(x, feed_dict={x:123})\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(123, dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.add(5, 2)  # 7\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.int32)\n",
    "b = tf.placeholder(tf.int32)\n",
    "x = tf.add(a, b)\n",
    "y = tf.sub(a, b) # 6\n",
    "z = tf.mul(a, b)  # 10\n",
    "w = tf.div(a, b) - 1\n",
    "with tf.Session() as sess:\n",
    "    # TODO: Feed the x tensor 123\n",
    "    output = sess.run(w, feed_dict={a:10, b:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = 10\n",
    "y = 2\n",
    "z = x/y - 1\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess: \n",
    "    z = sess.run(tf.div(x,y)-1)\n",
    "    \n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic classifier\n",
    "- logistic regression (isn't the video just talking about linear regression combined with softmax?)\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/WQsdr1EJgz8?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/WQsdr1EJgz8?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear functions in TensorFlow\n",
    "> The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. \n",
    "\n",
    "> As a reminder, we can write the output of the linear operation as\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a4d8b3_linear-equation/linear-equation.gif)\n",
    "> Here, \n",
    "- W is a **matrix** of the weights connecting two layers. \n",
    "- The output y, the input x, and the biases b are all **vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Bias in TensorFlow\n",
    "> **The goal of training a neural network is to modify weights and biases to best predict the labels**\n",
    "- In order to use weights and bias, you'll need a Tensor that can be modified. \n",
    "- This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. \n",
    "- This is where tf.Variable class comes in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.Variable()\n",
    "\n",
    "```python\n",
    "x = tf.Variable(5)\n",
    "```\n",
    "\n",
    "> The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. \n",
    "\n",
    "> This tensor stores its state in the session, so you must initialize the state of the tensor manually. \n",
    "\n",
    "> You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "```python\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "```\n",
    "\n",
    "> The `tf.global_variables_initializer()` call returns an operation that will initialize all TensorFlow variables from the graph. \n",
    "\n",
    "> You call the operation using a session to initialize all the variables as shown above. \n",
    "\n",
    "> Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "> Initializing the weights with random numbers from a normal distribution is good practice. \n",
    "\n",
    "> **Randomizing the weights helps the model from becoming stuck in the same place every time you train it**. \n",
    "\n",
    "> You'll learn more about this in the next lesson, when you study gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Similarly, choosing weights from a normal distribution **prevents any one weight from overwhelming other weights**. \n",
    "\n",
    "> You'll use the tf.truncated_normal() function to generate random numbers from a normal distribution.\n",
    "\n",
    "\n",
    "## tf.truncated_normal()\n",
    "```python\n",
    "\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "```\n",
    "\n",
    "> The tf.truncated_normal() function returns a tensor with random values from **a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "> Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias.\n",
    "\n",
    "> Let's use the simplest solution, setting the bias to 0.\n",
    "\n",
    "\n",
    "## tf.zeros()\n",
    "```python\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "```\n",
    "\n",
    "> The tf.zeros() function returns a tensor with all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier Quiz\n",
    "\n",
    "**A subset of the MNIST dataset**\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/582cf7a7_mnist-012/mnist-012.png)\n",
    "\n",
    "> You'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. \n",
    "\n",
    "> The above is a small sample of the data you'll be training on. \n",
    "\n",
    "> Notice how some of the 1s are written with a serif at the top and at different angles. \n",
    "\n",
    "> The similarities and differences will play a part in shaping the weights of the model.\n",
    "\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/582ce9ef_weights-0-1-2/weights-0-1-2.png)\n",
    "\n",
    "> **The images above are trained weights for each label (0, 1, and 2)**. \n",
    "\n",
    "> **The weights display the unique properties of each digit they have found**. \n",
    "\n",
    "> Complete this quiz to train your own weights using the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Open quiz.py.\n",
    "- Implement get_weights to return a tf.Variable of weights\n",
    "- Implement get_biases to return a tf.Variable of biases\n",
    "- Implement xW + b in the linear function\n",
    "- Open sandbox.py\n",
    "- Initialize all weights\n",
    "\n",
    "> Since xW in xW + b is matrix multiplication, you have to use the tf.matmul() function instead of tf.mul(). \n",
    "\n",
    "> Don't forget that order matters in matrix multiplication, so tf.matmul(a,b) is not the same as tf.matmul(b,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Sandbox Solution\n",
    "# Note: You can't run code in this tab\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65900117,  0.24243298,  0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do one-hot-encoder for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "lb.transform(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cross-entropy using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# ToDo: Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.mul(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize inputs and initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"760\" height=\"427\" src=\"https://www.youtube.com/embed/WaHQ9-UXIIg?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"760\" height=\"427\" src=\"https://www.youtube.com/embed/WaHQ9-UXIIg?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How can test data in evaluation sneek into training set? \n",
    "- if test set is used for evaluation for how good and how to adjust model based on training set\n",
    "- over time, the model is trained and adjusted under the guidance of test set\n",
    "- test set sneek in\n",
    "\n",
    "> This is why we need valdiation set from training set for evaluation and paramenter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"760\" height=\"427\" src=\"https://www.youtube.com/embed/jmq5hj_CWlM?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"760\" height=\"427\" src=\"https://www.youtube.com/embed/jmq5hj_CWlM?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to calc the gradient based on all data points at once is very inefficient and expensive to do\n",
    "\n",
    "> must randomize subset of data\n",
    "\n",
    "> each update or gradient calced from each batch would be bad, but over many iterations, it is efficient and good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/hMLUgM6kTp8?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/hMLUgM6kTp8?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum and Learning decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Don't understand Momentum at all**\n",
    "\n",
    "> learning_decay: to reduce learning rate as it approaches to optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/O3QYdmQjXds?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/O3QYdmQjXds?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Hyperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how fast and how well you learn\n",
    "- always lower your learning rate first \n",
    "- ADAGRAD: make things easier, automatically do inital learning rate, learning rate decay, momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/5a3-iIhdguc?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/5a3-iIhdguc?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching\n",
    "> In this section, you'll go over what mini-batching is and how to apply it in TensorFlow.\n",
    "\n",
    "> Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. \n",
    "\n",
    "> This provides the ability to train a model, even if a computer **lacks the memory to store the entire dataset**.\n",
    "\n",
    "> Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. \n",
    "\n",
    "> However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "> It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. \n",
    "\n",
    "> For each mini-batch, you train the network weights with gradient descent. \n",
    "\n",
    "> Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "> Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "# make features and labels same type\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# create Weights & bias placeholders, matrix and vector\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question 1\n",
    "Calculate the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data.\n",
    "\n",
    "You may have to look up how much memory a float32 requires, using this link.\n",
    "\n",
    "train_features Shape: (55000, 784) Type: float32\n",
    "\n",
    "train_labels Shape: (55000, 10) Type: float32\n",
    "\n",
    "weights Shape: (784, 10) Type: float32\n",
    "\n",
    "bias Shape: (10,) Type: float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Mini-batching\n",
    "> In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "> Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. \n",
    "\n",
    "> For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. \n",
    "\n",
    "> Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "> In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "\n",
    "> Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does None do here?\n",
    "\n",
    "> The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "> Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n",
    "> Question 2 : Use the parameters below, how many batches are there, and what is the last batch size?\n",
    "- features is (50000, 400)\n",
    "- labels is (50000, 10)\n",
    "- batch_size is 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "# from quiz import batches\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs\n",
    "> An epoch is a single forward and backward pass of the whole dataset. \n",
    "\n",
    "> This is used to increase the accuracy of the model without requiring more data. \n",
    "\n",
    "> This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "> The following TensorFlow code trains a model using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    \n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    \n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(  # try it out to find out the usage\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "# make sure they are in the same type\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels: create variable to contain matrix, without constraint on number of examples\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias: this is a NN without hidden layer, just from input layer to output layer\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits = xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# create placeholder for cost \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "\n",
    "# create place holder for updated amount of weights and biases\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "# create batches using helper function \n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # must init first\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle for each epoch\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            \n",
    "            # in each batch, calc updated amount of weights and biases\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # After training, Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code will output the following:\n",
    "\n",
    "Epoch: 0    - Cost: 11.0     Valid Accuracy: 0.204\n",
    "Epoch: 1    - Cost: 9.95     Valid Accuracy: 0.229\n",
    "Epoch: 2    - Cost: 9.18     Valid Accuracy: 0.246\n",
    "Epoch: 3    - Cost: 8.59     Valid Accuracy: 0.264\n",
    "Epoch: 4    - Cost: 8.13     Valid Accuracy: 0.283\n",
    "Epoch: 5    - Cost: 7.77     Valid Accuracy: 0.301\n",
    "Epoch: 6    - Cost: 7.47     Valid Accuracy: 0.316\n",
    "Epoch: 7    - Cost: 7.2      Valid Accuracy: 0.328\n",
    "Epoch: 8    - Cost: 6.96     Valid Accuracy: 0.342\n",
    "Epoch: 9    - Cost: 6.73     Valid Accuracy: 0.36 \n",
    "Test Accuracy: 0.3801000118255615\n",
    "\n",
    "> Each epoch attempts to move to a lower cost, leading to better accuracy.\n",
    "\n",
    "> This model continues to improve accuracy up to Epoch 9. Let's increase the number of epochs to 100.\n",
    "\n",
    "...\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.86\n",
    "Epoch: 80   - Cost: 0.11     Valid Accuracy: 0.869\n",
    "Epoch: 81   - Cost: 0.109    Valid Accuracy: 0.869\n",
    "....\n",
    "Epoch: 85   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 86   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 87   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 88   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 89   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 90   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 91   - Cost: 0.104    Valid Accuracy: 0.869\n",
    "Epoch: 92   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 93   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 94   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 95   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 96   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 97   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 98   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Epoch: 99   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.8696000006198883\n",
    "\n",
    "> From looking at the output above, you can see the model doesn't increase the validation accuracy after epoch 80. \n",
    "\n",
    "> Let's see what happens when we increase the learning rate.\n",
    "\n",
    "learn_rate = 0.1\n",
    "\n",
    "Epoch: 76   - Cost: 0.214    Valid Accuracy: 0.752\n",
    "Epoch: 77   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "Epoch: 78   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "...\n",
    "Epoch: 85   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 86   - Cost: 0.209    Valid Accuracy: 0.756\n",
    "Epoch: 87   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 88   - Cost: 0.208    Valid Accuracy: 0.756\n",
    "Epoch: 89   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 90   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 91   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 92   - Cost: 0.204    Valid Accuracy: 0.756\n",
    "Epoch: 93   - Cost: 0.206    Valid Accuracy: 0.756\n",
    "Epoch: 94   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 95   - Cost: 0.2974   Valid Accuracy: 0.756\n",
    "Epoch: 96   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 97   - Cost: 0.2996   Valid Accuracy: 0.756\n",
    "Epoch: 98   - Cost: 0.203    Valid Accuracy: 0.756\n",
    "Epoch: 99   - Cost: 0.2987   Valid Accuracy: 0.756\n",
    "Test Accuracy: 0.7556000053882599\n",
    "\n",
    "> Looks like the learning rate was increased too much. \n",
    "\n",
    "> The final accuracy was lower, and it stopped improving earlier. \n",
    "\n",
    "> Let's stick with the previous learning rate, but change the number of epochs to 80.\n",
    "\n",
    "Epoch: 65   - Cost: 0.122    Valid Accuracy: 0.868\n",
    "Epoch: 66   - Cost: 0.121    Valid Accuracy: 0.868\n",
    "Epoch: 67   - Cost: 0.12     Valid Accuracy: 0.868\n",
    "Epoch: 68   - Cost: 0.119    Valid Accuracy: 0.868\n",
    "Epoch: 69   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 70   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 71   - Cost: 0.117    Valid Accuracy: 0.868\n",
    "Epoch: 72   - Cost: 0.116    Valid Accuracy: 0.868\n",
    "Epoch: 73   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 74   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 75   - Cost: 0.114    Valid Accuracy: 0.868\n",
    "Epoch: 76   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 77   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 78   - Cost: 0.112    Valid Accuracy: 0.868\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.868\n",
    "Epoch: 80   - Cost: 0.111    Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.86909999418258667\n",
    "\n",
    "> The accuracy only reached 0.86, but that could be because the learning rate was too high. \n",
    "\n",
    "> Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy.\n",
    "\n",
    "> In the upcoming TensorFLow Lab, you'll get the opportunity to choose your own learning rate, epoch count, and batch size to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
